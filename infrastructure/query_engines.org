|                             | AWS Redshift                 | Google BigQuery          | Spark (Databricks)        |
|-----------------------------+------------------------------+--------------------------+---------------------------|
| In production at Nubank     | Since mid 2016               | No                       | No                        |
| Used for development        | No                           | No                       | Yes[fn:2]                 |
| Metabase                    | Can connect                  | Limited support[fn:1]    | Can not connect           |
| Looker                      | Can connect                  | Can connect              | Can connect               |
| Jupyter Notebook/Pandas     | Easy, using belomonte        | Easy, pd.read_gbq        | Embedded notebooks[fn:12] |
| DBeaver                     | Easy, lot of people doing it | Can connect              | Can't connect             |
| Pricing model               | Fixed cluster                | Pay per query            | Auto-scaling cluster      |
| Runs on                     | AWS                          | Google Cloud             | AWS                       |
| We have experience          | Yes                          | No                       | Yes                       |
| Loads data from             | S3 (Avro)                    | GCS (Avro/Parquet[fn:8]) | Agnostic[fn:3]            |
| SQL Dialect                 | Very old PostgreSQL (8.0.2)  | Standard SQL             | Spark SQL                 |
| Complex schema support      | Limited JSON support         | Awesome[fn:13]           | Sufficient[fn:14]         |
| Query start delay/overhead  | Relatively small[fn:15]      | Relatively small[fn:16]  | Relatively large[fn:17]   |
| Caching                     | Data caching                 | Query caching            | Data caching[fn:18]       |
| Future potential[fn:4]      | Doesn't look promising[fn:5] | Most promising[fn:6]     | Promising[fn:7]           |
| SLA in-place?               | Great, existing AWS support  | None yet                 | Good, support is so-so    |
| Data load process           | Big overhead[fn:9]           | Big overhead[fn:10]      | Small overhead[fn:11]     |
| BigDecimal support          | Yes                          | No, but is in alpha      | Yes                       |
| Concurrent query support    | TODO                         | TODO                     | TODO                      |
| Performance characteristics | TODO                         | TODO                     | TODO                      |

[fn:1] It doesn't support nested schemas
[fn:2] Through notebooks
[fn:3] Can be any Hive-registered table
[fn:4] Hot new thing
[fn:5] AWS isn't adding great features to the Redshift core--just bandaid like
Spectrum
[fn:6] Google is pushing hard on its Google Cloud Platform
[fn:7] Spark is an active open-source project
[fn:8] Parquet is in beta
[fn:9] Requires a system that generates manifest files to S3 and manage the
connection pooling
[fn:10] Currently requires a transfer of data from AWS to GCP, after that
requires some lambda style function that triggers load into BigQuery
[fn:11] Just needs to register a Hive table for it to be available for queries
[fn:12] Databricks has their own web based notebook implementation that runs
Scala, Python, R, and SQL that you are supposed to use if you want to benefit
from auto scaling clusters
[fn:13] Google BigQuery has a lot of functions for arrays:
https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays
[fn:14] Spark has sufficient support to create and unnest collections, but is
probably more verbose than BigQuery
[fn:15] Smaller overhead than reading from block storage like S3 because data is
stored on disk:
https://docs.aws.amazon.com/redshift/latest/dg/c_redshift_system_overview.html
[fn:16] Smaller overhead than reading from block storage like S3 because of
BigQuery's distributed filesystem Colossus:
https://cloud.google.com/blog/big-data/2016/04/inside-capacitor-bigquerys-next-generation-columnar-storage-format
[fn:17] Databricks queries files that are on block storage like S3, so IO
performance is worse than disk storage like Redshift and BigQuery
[fn:18] Databricks is improving its IO caching and its configurable:
https://docs.databricks.com/user-guide/databricks-io-cache.html
