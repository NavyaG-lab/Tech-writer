---
owner: "#data-infra"
---

<!-- markdwonlint-disable-file -->

# Serving of Datasets in the Staging Environment

There are a few use cases for the users of the ETL to test data to be served in the staging environment.

For that, there's a new alternative which is called `datainfra stage-serve`, to manually add Avro data files to the serving layer on staging.

## How does it work

The regular production serving layer reads an Avro file generated by Itaipu and serves it with the current dataset serving options. Either loading it into ConradoDB or propagating it into Kafka, for usage in further microservices.

`stage-serve` will read an Avro file generated by the users and call `metapod`, our metadata store, to serve it into stage Conrado and/or Kafka.

This is done by running a CLI command that takes an Avro file on s3 and the associated [logical type schema](/glossary.md#logical-type-schema) encoded as a json. The logical type schema describes the schema of the Avro file using Nubank's custom type system. In Scala code we define this schema by having our SparkOps extend `DeclaredSchema` and defining the `attributes` field. Since we are working from the command line, we need to write this manually as a json file.

With the Avro file and the schema file, the CLI command validates the logical type schema against the Avro file, and tells our metadata store about your new dataset.

## Preparing the data
In a nutshell, place your Avro file or directory on the s3 bucket for stage serving layer (`s3://nu-spark-metapod-ephemeral-staging/me/my-dataset.avro`) and place your logical type json schema in the same directory and name it `schema.json` (`s3://nu-spark-metapod-ephemeral-staging/me/schema.json`)

**The data ingestion can only be made from this S3 bucket `s3://nu-spark-metapod-ephemeral-staging`.**

In detail:

### The Avro file

- Create an Avro file from whatever tool / source you want.
- First, place it on `s3` somewhere that you have permissions to access (i.e `s3://nu-tmp/me/my-dataset`).
- Open it up on databricks and take a look at the schema of the resulting dataframe:

```scala
val df = spark.read.format("avro").load("dbfs:/mnt/nu-tmp/me/my-dataset")
df.schema
// results in:
StructType(
  StructField(id,StringType,true),
  StructField(example_boolean,BooleanType,true),
  StructField(example_booleans,ArrayType(BooleanType,true),true),
  StructField(example_date,DateType,true),
  StructField(example_dates,ArrayType(DateType,true),true),
  StructField(example_decimal,DecimalType(38,9),true),
  StructField(example_decimals,ArrayType(DecimalType(38,9),true),true),
  StructField(example_double,DoubleType,true),
  StructField(example_doubles,ArrayType(DoubleType,true),true),
  StructField(example_enum,StringType,true),
  StructField(example_enums,ArrayType(StringType,true),true),
  StructField(example_integer,LongType,true),
  StructField(example_integers,ArrayType(LongType,true),true),
  StructField(example_string,StringType,true),
  StructField(example_strings,ArrayType(StringType,true),true),
  StructField(example_timestamp,TimestampType,true),
  StructField(example_timestamps,ArrayType(TimestampType,true),true),
  StructField(example_uuid,StringType,true),
  StructField(example_uuids,ArrayType(StringType,true),true)
)
```

You'll see that the Avro schema loses some type information, for instance, UUIDs are stored as strings.
This is why we define our own logical type schema.

### The logical type schema

The logical type schema describes the schema of the dataset using our own internal schema language instead of that of Avro.
They differ a little bit, hence we don't have a tool to generate them automatically.
That said, we do validate that the Avro and logical type schemas match up before committing the dataset to the serving layer.

**Important: The Logical type `INTEGER` actually maps to the Spark type `LongType`.**

[Here is a full example of the logical type schema](../manual_series_schema.json) for the dataframe above.
Please keep in mind that, if you have nullable fields, these have to be explicitly declared in the logical schema.

It looks roughly like:

```json
{
    "attributes": [
      {
        "name": "example_number",
        "primaryKey": false,
        "nullable": true,
        "logicalType": "DOUBLE",
        "logicalSubType": null
      },
      {
        "name": "id",
        "primaryKey": true,
        "nullable": false,
        "logicalType": "UUID",
        "logicalSubType": null
      },
      ...
      ]
}
```

Prepare this file, name it `schema.json`, and place it in the same directory on s3 that your Avro file is at (i.e `s3://nu-spark-metapod-ephemeral-staging/me/schema.json`).

## Before appending

First you will need to have docker installed, download [Here](https://download.docker.com/mac/stable/Docker.dmg). Docker is a set of coupled software-as-a-service and platform-as-a-service products that use operating-system-level virtualization to develop and deliver software in packages called containers [Wikipedia](https://en.wikipedia.org/wiki/Docker_(software)). Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating-system kernel and are thus more lightweight than virtual machines.

Since appending involves talking directly to `metapod`, you will need the `metapod-admin` scope for your AWS user on staging. Use `nu-<country> sec scope show <your-firstname.your-lastname> --env=staging` to see if you have the `metapod-admin` scope. If you don't, ask for it using the access request form pinned to the `#access-request` channel. After getting it you may need to run `nu-<country> auth get-refresh-token --env prod --country <country>`. Where `<country>` is the country where you are running your serving request (eg: br, mx).

## Running

```
nu-<country> datainfra stage-serve my-dataset-name s3://nu-spark-metapod-ephemeral-staging/my-name/my-dataset
```

* If there is any error, follow the command instructions to find the error logs on Splunk

  * Otherwise, The data should be served on Conrado DB and/or Kafka

  * You can also follow the command's instructions to find the logs about successfully served data on Splunk

## Troubleshooting

For any other questions, please check `#squad-data-infra` channel on slack.
